{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6687a1c-11ce-47b5-8cb1-320a84f6c6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleksandra/Documents/hackathon/knum_venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "import json\n",
    "from ipynb.fs.full.read_in_data import read_in_data\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7bae4cf-b700-47de-bccc-73c82ca56bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>bbox</th>\n",
       "      <th>category_id</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>[665, 448, 206, 174]</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGBA size=206x174 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>[475, 186, 236, 186]</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGBA size=236x186 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>[962, 76, 229, 217]</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGBA size=229x217 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>[778, 250, 163, 135]</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGBA size=163x135 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>[184, 377, 244, 194]</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGBA size=244x194 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  image_id                  bbox  category_id  \\\n",
       "12  12         4  [665, 448, 206, 174]            8   \n",
       "13  13         4  [475, 186, 236, 186]            8   \n",
       "14  14         4   [962, 76, 229, 217]            8   \n",
       "15  15         4  [778, 250, 163, 135]            8   \n",
       "16  16         4  [184, 377, 244, 194]            8   \n",
       "\n",
       "                                                  img  \n",
       "12  <PIL.Image.Image image mode=RGBA size=206x174 ...  \n",
       "13  <PIL.Image.Image image mode=RGBA size=236x186 ...  \n",
       "14  <PIL.Image.Image image mode=RGBA size=229x217 ...  \n",
       "15  <PIL.Image.Image image mode=RGBA size=163x135 ...  \n",
       "16  <PIL.Image.Image image mode=RGBA size=244x194 ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../public_dataset/reference_images_part1.json\") as f:\n",
    "    ref_json = json.load(f)\n",
    "\n",
    "dir_str = \"../public_dataset/reference_images_part1\"\n",
    "data = read_in_data(dir_str, ref_json)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1865598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Set, Tuple, Union\n",
    "\n",
    "class ViTForImageClassification(ViTForImageClassification):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__(config)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        interpolate_pos_encoding: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.vit(\n",
    "            pixel_values,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            interpolate_pos_encoding=interpolate_pos_encoding,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        return outputs[0][:, 0, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0fb2d8a-6c9c-4d97-b82b-6a00227c8f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "image = data.img[12]\n",
    "\n",
    "image_ = Image.new(\"RGB\", image.size, (255, 255, 255))\n",
    "image_.paste(image, mask=image.split()[3]) \n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "\n",
    "inputs = feature_extractor(images=image_, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs.shape) # co to jest 197??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb54493-bfc1-41d5-989d-73f16f51ecba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KNUM hackathon",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
